{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db0437b3-b057-4437-bfd6-cee67277fbbb",
   "metadata": {},
   "source": [
    "# Machine Learning 2 Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21169c73-ff7a-4804-83f8-b7c633da72fd",
   "metadata": {},
   "source": [
    "## Junyoung Jung "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20378f9d-738b-4c5d-a8ec-62bff2c080dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim import SGD\n",
    "import torch.nn\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172d84a7-ad7b-45c8-9824-4c67865cf3a5",
   "metadata": {},
   "source": [
    "### a) Load CIFAR10 dataset as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cee4218-a75c-4431-9c07-f78f630c281b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "trainset = datasets.CIFAR10(root='./CIFARdata', train=True, download=True, transform=transforms.ToTensor())\n",
    "testset = datasets.CIFAR10(root='./CIFARdata', train=False, download=True, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15dc6170-fa48-4593-bbaf-7a4ad75d8a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_dict = {\n",
    "    0: 'airplane',\n",
    "    1: 'automobile',\n",
    "    2: 'bird',\n",
    "    3: 'cat',\n",
    "    4: 'deer',\n",
    "    5: 'dog',\n",
    "    6: 'frog',\n",
    "    7: 'horse',\n",
    "    8: 'ship',\n",
    "    9: 'truck'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904695f1-592e-4920-823b-182bdb91fffe",
   "metadata": {},
   "source": [
    "### b) Visualize at least one image for each class. You may need to look into how dataset is implemented in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "884cf798-5bee-4f83-bb84-83218d3d3fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_image(data):\n",
    "    figure = plt.figure(figsize=(8, 8))\n",
    "    cols, rows = 3, 3\n",
    "    for i in range(1, cols * rows + 1):\n",
    "        sample_idx = torch.randint(len(data), size=(1,)).item()\n",
    "        img, label = data[sample_idx]\n",
    "        figure.add_subplot(rows, cols, i)\n",
    "        plt.title(labels_dict[label])\n",
    "        plt.axis(\"off\")\n",
    "        plt.imshow(img.T.squeeze(), cmap=\"gray\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0d0be0-8b86-45a2-9e7f-1bd7a79e6078",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_image(trainset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32723e8c-0478-4a2d-afd0-47025222d086",
   "metadata": {},
   "source": [
    "### c) Split the trainset into training set and validation set with 90% : 10% ratio. Implement dataloaders for CIFAR10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31716bcc-bb19-45bb-abad-fb4e2e59e5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_size = len(trainset)\n",
    "train_size = int(0.9 * total_size)\n",
    "val_size = int(0.1 * total_size)\n",
    "\n",
    "train_dataset, val_dataset = random_split(trainset, [train_size, val_size])\n",
    "\n",
    "# data shape[0] = (1, 3, 32, 32)\n",
    "# label shape[0] = (1)\n",
    "\n",
    "trainloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "valloader = DataLoader(val_dataset, shuffle=True)\n",
    "testloader = DataLoader(testset, shuffle=False)\n",
    "\n",
    "anyclass1, anyclass2 = 3, 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c1fb2e-ae98-4314-ba0a-fd4d47d55c9e",
   "metadata": {},
   "source": [
    "### d) Choose any two classes. Then, make a SVM classifier (implement a loss function yourself. Do not use PyTorch implementations of loss functions.) and its training/validation/evaluation code to perform binary classification between those two classes.\n",
    "\n",
    "\n",
    "### e) Train for 10 epochs with batch size 64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86067969-b9fa-4c15-945c-b6133d818b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class svm(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(svm, self).__init__()\n",
    "        self.fc = torch.nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        fd = self.fc(x)\n",
    "        return fd\n",
    "\n",
    "    def hingeloss(self, data, label):\n",
    "        zero = torch.zeros(1)\n",
    "        # for i in range(data_size):\n",
    "        #     ans = -1\n",
    "        #     if data[i] is not ans_label1 or ans_label2:\n",
    "        #         continue\n",
    "        #     else:\n",
    "        #         if data[i] is ans_label1:\n",
    "        #             ans = 1\n",
    "        #             result = torch.max(zero, 1 - ans * (w.T * data - b))\n",
    "        #         if data[i] is ans_label2:\n",
    "        #             result = torch.max(zero, 1 - ans * (w.T * data - b))\n",
    "        loss = torch.mean(torch.max(zero, 1 - label * data))\n",
    "        return loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b27112-f9f2-4409-ac75-ac9b3fa4df7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = svm()\n",
    "epoch = 10\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "optimizer = SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb3dfd4-8ef6-4491-8c95-c0916548f870",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer):\n",
    "    size = train_size\n",
    "    n = 1\n",
    "    model.train()\n",
    "\n",
    "    for batch, (X, y) in enumerate(trainloader):\n",
    "        label=0\n",
    "        for i in y:\n",
    "            if i == anyclass1:\n",
    "                label = -1\n",
    "            elif i == anyclass2:\n",
    "                label = 1\n",
    "            else:\n",
    "                continue\n",
    "        if label != anyclass1 and label != anyclass2: continue\n",
    "        optimizer.zero_grad()\n",
    "        prediction = model(X)\n",
    "        loss = model.hingeloss(prediction, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch % batch_size == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"Training loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646e9f61-c83e-4117-a51d-8914bbbf036e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, optimizer):\n",
    "    size = val_size\n",
    "    n = 1\n",
    "    for batch, (X, y) in enumerate(valloader):\n",
    "        start_time = time.time()\n",
    "        if y == anyclass1:\n",
    "            y = -1\n",
    "        elif y == anyclass2:\n",
    "            y = 1\n",
    "        else:\n",
    "            continue\n",
    "        optimizer.zero_grad()\n",
    "        prediction = model.classifier(X)\n",
    "        loss = model.hingeloss(prediction, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch % batch_size == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"Training loss: {loss:>7f}  [{current:>5d}/{size:>5d}] at step{n}\")\n",
    "\n",
    "    n += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6fc253-e140-4b4c-b3ef-2d49f77db921",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, optimizer):\n",
    "    size = val_size\n",
    "    n = 1\n",
    "    for batch, (X, y) in enumerate(testloader):\n",
    "        start_time = time.time()\n",
    "        if y == anyclass1:\n",
    "            y = -1\n",
    "        elif y == anyclass2:\n",
    "            y = 1\n",
    "        else:\n",
    "            continue\n",
    "        optimizer.zero_grad()\n",
    "        prediction = model.classifier(X)\n",
    "        loss = model.hingeloss(prediction, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch % batch_size == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"Training loss: {loss:>7f}  [{current:>5d}/{size:>5d}] at step{n}\")\n",
    "\n",
    "    n += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac441ae0-f853-4511-b1e4-67d46c535428",
   "metadata": {},
   "source": [
    "### f) Perform data normalization. You may need to look into how to use datasets in PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac5e50d-cda2-496f-ae25-427218d4a129",
   "metadata": {},
   "source": [
    "### g) Again, train for 10 epochs with batch size 64 after data normalization. Write down your observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5902611-8851-45bf-8f9a-e07d5128e20c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
